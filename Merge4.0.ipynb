{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b162e073-bc06-459b-9708-78c01c013aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "üöÄ INICIANDO PIPELINE COMPLETO DE DADOS üöÄ\n",
      "======================================================\n",
      "Iniciando Etapa 1: Limpeza do arquivo de audi√™ncia bruto...\n",
      "‚úÖ Etapa 1 conclu√≠da: Audi√™ncia bruta limpa e formatada em mem√≥ria.\n",
      "\n",
      "Iniciando Etapa 2: Processamento do Rundown (PDF)...\n",
      "‚úÖ Etapa 2 conclu√≠da: Rundown (PDF) processado com sucesso.\n",
      "\n",
      "Iniciando Etapa 3: Expans√£o e Merge dos dados...\n",
      "Realizando o merge dos dados...\n",
      "‚úÖ Etapa 3 conclu√≠da: Merge finalizado com sucesso.\n",
      "\n",
      "======================================================\n",
      "üèÜ PIPELINE FINALIZADO COM SUCESSO! üèÜ\n",
      "Arquivo final salvo em: 'audiencia_espelho_merged_segundos.csv'\n",
      "======================================================\n",
      "\n",
      "Visualiza√ß√£o das 5 primeiras linhas do resultado:\n",
      "    HORA GLOBO RECORD   SBT Conte√∫do de TV/V√≠deo sem refer√™ncia  \\\n",
      "0  08:40  7,72   3,83  1,20                                8,23   \n",
      "1  08:40  7,72   3,83  1,20                                8,23   \n",
      "2  08:40  7,72   3,83  1,20                                8,23   \n",
      "3  08:40  7,72   3,83  1,20                                8,23   \n",
      "4  08:40  7,72   3,83  1,20                                8,23   \n",
      "\n",
      "  Total Ligados Especial datetime_segundo_audiencia Bloco Tipo Retranca  \\\n",
      "0                  26,52        1900-01-01 08:40:00   NaN  NaN      NaN   \n",
      "1                  26,52        1900-01-01 08:40:01   NaN  NaN      NaN   \n",
      "2                  26,52        1900-01-01 08:40:02   NaN  NaN      NaN   \n",
      "3                  26,52        1900-01-01 08:40:03   NaN  NaN      NaN   \n",
      "4                  26,52        1900-01-01 08:40:04   NaN  NaN      NaN   \n",
      "\n",
      "  inicio_lauda fim_lauda  Ordem  \n",
      "0          NaN       NaN    NaN  \n",
      "1          NaN       NaN    NaN  \n",
      "2          NaN       NaN    NaN  \n",
      "3          NaN       NaN    NaN  \n",
      "4          NaN       NaN    NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_26344\\1534761766.py:108: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  lambda t: pd.date_range(start=f\"1900-01-01 {t}\", periods=60, freq='S')\n",
      "C:\\Temp\\ipykernel_26344\\1534761766.py:123: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  lambda row: pd.date_range(start=row['inicio_lauda_dt'], end=row['fim_lauda_dt'], freq='S'), axis=1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# ETAPA 1: FUN√á√ÉO PARA LIMPAR A AUDI√äNCIA BRUTA\n",
    "# ==============================================================================\n",
    "\n",
    "def limpar_audiencia_bruta(caminho_arquivo_bruto):\n",
    "    \"\"\"\n",
    "    L√™ o arquivo de audi√™ncia bruto, remove as 6 primeiras e a √∫ltima linha,\n",
    "    seleciona, renomeia as colunas e retorna um DataFrame limpo.\n",
    "    \"\"\"\n",
    "    print(\"Iniciando Etapa 1: Limpeza do arquivo de audi√™ncia bruto...\")\n",
    "    \n",
    "    try:\n",
    "        # Dicion√°rio que define as colunas a serem mantidas e seus novos nomes\n",
    "        colunas_para_renomear = {\n",
    "            'MIN': 'HORA',\n",
    "            'TV GAZETA': 'GLOBO',\n",
    "            'TV VITORIA': 'RECORD',\n",
    "            'TV SIM': 'SBT',\n",
    "            'CONTE√öDO TV/V√çDEO S/ REFER√äNCIA': 'Conte√∫do de TV/V√≠deo sem refer√™ncia',\n",
    "            'TOTAL LIGADOS': 'Total Ligados Especial'\n",
    "        }\n",
    "\n",
    "        # L√™ o arquivo original, pulando as 6 primeiras linhas\n",
    "        # O skipfooter exige a engine 'python'\n",
    "        df = pd.read_csv(caminho_arquivo_bruto, sep=';', skiprows=6, skipfooter=1, engine='python')\n",
    "\n",
    "        # Pega a lista das colunas originais que queremos manter\n",
    "        colunas_originais = list(colunas_para_renomear.keys())\n",
    "\n",
    "        # Filtra o DataFrame, mantendo apenas as colunas desejadas\n",
    "        df_selecionado = df[colunas_originais]\n",
    "\n",
    "        # Renomeia as colunas e cria o DataFrame final\n",
    "        df_limpo = df_selecionado.rename(columns=colunas_para_renomear)\n",
    "\n",
    "        print(\"‚úÖ Etapa 1 conclu√≠da: Audi√™ncia bruta limpa e formatada em mem√≥ria.\")\n",
    "        return df_limpo\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERRO na Etapa 1: O arquivo de entrada '{caminho_arquivo_bruto}' n√£o foi encontrado.\")\n",
    "        return None\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå ERRO na Etapa 1: Uma ou mais colunas para renomear n√£o foram encontradas: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ocorreu um erro inesperado na Etapa 1: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==============================================================================\n",
    "# ETAPA 2: FUN√á√ÉO PARA PROCESSAR O RUNDOWN (PDF)\n",
    "# ==============================================================================\n",
    "\n",
    "def processar_rundown_pdf(caminho_pdf):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o est√°vel para extrair os dados do rundown (espelho) de um PDF.\n",
    "    \"\"\"\n",
    "    print(\"\\nIniciando Etapa 2: Processamento do Rundown (PDF)...\")\n",
    "    if not os.path.exists(caminho_pdf):\n",
    "        print(f\"‚ùå ERRO na Etapa 2: O arquivo PDF n√£o foi encontrado em '{caminho_pdf}'\")\n",
    "        return None\n",
    "\n",
    "    dados_finais = []\n",
    "    bloco_numero = 1\n",
    "    with pdfplumber.open(caminho_pdf) as pdf:\n",
    "        for pagina in pdf.pages:\n",
    "            tabelas = pagina.extract_tables()\n",
    "            for tabela in tabelas:\n",
    "                for linha in tabela:\n",
    "                    if linha and linha[0] and isinstance(linha[0], str) and 'Break' in linha[0]:\n",
    "                        bloco_numero += 1\n",
    "                        continue\n",
    "                    if len(linha) >= 10 and linha[-1] and isinstance(linha[-1], str) and linha[-1].isdigit():\n",
    "                        dados_finais.append({\n",
    "                            'Bloco': f\"Bloco {bloco_numero:02d}\", 'Tipo': linha[1], 'Retranca': linha[2],\n",
    "                            'In√≠cio (Lauda)': linha[6], 'Fim (Lauda)': linha[7], 'Ordem': int(linha[9])\n",
    "                        })\n",
    "    if not dados_finais:\n",
    "        print(\"‚ö†Ô∏è Aten√ß√£o na Etapa 2: Nenhum dado de lauda v√°lido foi extra√≠do do PDF.\")\n",
    "        return None\n",
    "    df = pd.DataFrame(dados_finais).sort_values(by='Ordem').reset_index(drop=True)\n",
    "    print(\"‚úÖ Etapa 2 conclu√≠da: Rundown (PDF) processado com sucesso.\")\n",
    "    return df\n",
    "\n",
    "# ==============================================================================\n",
    "# ETAPA 3: FUN√á√ÉO PARA FAZER O MERGE FINAL\n",
    "# ==============================================================================\n",
    "\n",
    "def fazer_merge_final(df_espelho_orig, df_audiencia_limpa):\n",
    "    \"\"\"\n",
    "    Recebe os DataFrames do espelho e da audi√™ncia limpa,\n",
    "    e realiza a expans√£o e o merge em n√≠vel de segundo.\n",
    "    \"\"\"\n",
    "    print(\"\\nIniciando Etapa 3: Expans√£o e Merge dos dados...\")\n",
    "    \n",
    "    # --- 3.1: Processando e expandindo audi√™ncia ---\n",
    "    df_minamin = df_audiencia_limpa.copy()\n",
    "    df_minamin['HORA_base'] = pd.to_datetime(df_minamin['HORA'], format='%H:%M', errors='coerce').dt.time\n",
    "    df_minamin.dropna(subset=['HORA_base'], inplace=True)\n",
    "    if df_minamin.empty:\n",
    "        print(\"‚ùå ERRO na Etapa 3: O dataframe de audi√™ncia ficou vazio ap√≥s a limpeza. Verifique o formato da hora no arquivo bruto.\")\n",
    "        return None\n",
    "    df_minamin_segundos = df_minamin.copy()\n",
    "    df_minamin_segundos['datetime_segundo_audiencia'] = df_minamin_segundos['HORA_base'].apply(\n",
    "        lambda t: pd.date_range(start=f\"1900-01-01 {t}\", periods=60, freq='S')\n",
    "    )\n",
    "    df_minamin_segundos = df_minamin_segundos.explode('datetime_segundo_audiencia')\n",
    "    \n",
    "    # --- 3.2: Processando e expandindo espelho ---\n",
    "    df_espelho = df_espelho_orig.copy()\n",
    "    df_espelho.rename(columns={'In√≠cio (Lauda)': 'inicio_lauda', 'Fim (Lauda)': 'fim_lauda'}, inplace=True)\n",
    "    df_espelho['inicio_lauda_dt'] = pd.to_datetime('1900-01-01 ' + df_espelho['inicio_lauda'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "    df_espelho['fim_lauda_dt'] = pd.to_datetime('1900-01-01 ' + df_espelho['fim_lauda'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "    df_espelho.dropna(subset=['inicio_lauda_dt', 'fim_lauda_dt'], inplace=True)\n",
    "    df_espelho = df_espelho[df_espelho['fim_lauda_dt'] >= df_espelho['inicio_lauda_dt']].copy()\n",
    "    if df_espelho.empty:\n",
    "        print(\"‚ö†Ô∏è Aten√ß√£o na Etapa 3: O dataframe do espelho ficou vazio ap√≥s a limpeza.\")\n",
    "        return None\n",
    "    df_espelho['datetime_segundo_espelho'] = df_espelho.apply(\n",
    "        lambda row: pd.date_range(start=row['inicio_lauda_dt'], end=row['fim_lauda_dt'], freq='S'), axis=1\n",
    "    )\n",
    "    df_espelho_segundos = df_espelho.explode('datetime_segundo_espelho').drop_duplicates(subset=['datetime_segundo_espelho'], keep='first')\n",
    "\n",
    "    # --- 3.3: Merge Final ---\n",
    "    print(\"Realizando o merge dos dados...\")\n",
    "    df_merged_segundos = pd.merge(\n",
    "        df_minamin_segundos, df_espelho_segundos,\n",
    "        left_on='datetime_segundo_audiencia', right_on='datetime_segundo_espelho', how='left'\n",
    "    )\n",
    "\n",
    "    # --- 3.4: Limpeza e Salvamento ---\n",
    "    colunas_para_remover = ['HORA_base', 'datetime_segundo_espelho', 'inicio_lauda_dt', 'fim_lauda_dt']\n",
    "    df_merged_segundos.drop(columns=colunas_para_remover, inplace=True, errors='ignore')\n",
    "    \n",
    "    print(\"‚úÖ Etapa 3 conclu√≠da: Merge finalizado com sucesso.\")\n",
    "    return df_merged_segundos\n",
    "\n",
    "# ==============================================================================\n",
    "# PONTO DE PARTIDA PRINCIPAL DO SCRIPT\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Defina os NOMES DOS ARQUIVOS DE ENTRADA aqui ---\n",
    "    ARQUIVO_AUDIENCIA_BRUTO = \"Realtime_bruto.csv\"\n",
    "    ARQUIVO_RUNDOWN_PDF = \"rundown-display.pdf\"\n",
    "    \n",
    "    # --- Defina o NOME DO ARQUIVO DE SA√çDA aqui ---\n",
    "    ARQUIVO_SAIDA_FINAL = \"audiencia_espelho_merged_segundos.csv\"\n",
    "\n",
    "    print(\"======================================================\")\n",
    "    print(\"üöÄ INICIANDO PIPELINE COMPLETO DE DADOS üöÄ\")\n",
    "    print(\"======================================================\")\n",
    "    \n",
    "    # Roda a Etapa 1\n",
    "    df_audiencia_limpa = limpar_audiencia_bruta(ARQUIVO_AUDIENCIA_BRUTO)\n",
    "    \n",
    "    # Roda a Etapa 2\n",
    "    df_espelho_gerado = processar_rundown_pdf(ARQUIVO_RUNDOWN_PDF)\n",
    "    \n",
    "    # Roda a Etapa 3 apenas se as etapas anteriores foram bem-sucedidas\n",
    "    if df_audiencia_limpa is not None and df_espelho_gerado is not None:\n",
    "        df_final = fazer_merge_final(df_espelho_gerado, df_audiencia_limpa)\n",
    "        \n",
    "        if df_final is not None and not df_final.empty:\n",
    "            # Salva o resultado final\n",
    "            df_final.to_csv(ARQUIVO_SAIDA_FINAL, index=False, encoding='utf-8-sig')\n",
    "            print(\"\\n======================================================\")\n",
    "            print(f\"üèÜ PIPELINE FINALIZADO COM SUCESSO! üèÜ\")\n",
    "            print(f\"Arquivo final salvo em: '{ARQUIVO_SAIDA_FINAL}'\")\n",
    "            print(\"======================================================\")\n",
    "            print(\"\\nVisualiza√ß√£o das 5 primeiras linhas do resultado:\")\n",
    "            print(df_final.head())\n",
    "        else:\n",
    "            print(\"\\nO pipeline foi interrompido na Etapa 3 porque o resultado do merge foi vazio.\")\n",
    "    else:\n",
    "        print(\"\\nO pipeline foi interrompido porque uma das etapas iniciais (1 ou 2) falhou.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16b44e3-892d-4f1d-a9e5-edf3cac43601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Espelho)",
   "language": "python",
   "name": "espelho"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
