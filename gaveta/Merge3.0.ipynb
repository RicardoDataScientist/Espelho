{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ad5fa68-d302-4069-aadb-8251a48e3cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando pipeline de processamento e merge...\n",
      "PDF do rundown processado com sucesso!\n",
      "Arquivo de audi√™ncia 'Realtime_clean.csv' carregado com sucesso!\n",
      "Processando dados de audi√™ncia...\n",
      "Processando dados do espelho (rundown)...\n",
      "Realizando o merge dos dados...\n",
      "\n",
      "PROCESSO FINALIZADO! üèÜ\n",
      "Arquivo mergeado salvo em: audiencia_espelho_merged_segundos.csv\n",
      "\n",
      "Visualiza√ß√£o das primeiras linhas do dataframe final:\n",
      "    HORA GLOBO RECORD   SBT Conte√∫do de TV/V√≠deo sem refer√™ncia  \\\n",
      "0  08:40  7,72   3,83  1,20                                8,23   \n",
      "1  08:40  7,72   3,83  1,20                                8,23   \n",
      "2  08:40  7,72   3,83  1,20                                8,23   \n",
      "3  08:40  7,72   3,83  1,20                                8,23   \n",
      "4  08:40  7,72   3,83  1,20                                8,23   \n",
      "\n",
      "  Total Ligados Especial datetime_segundo_audiencia Bloco Tipo Retranca  \\\n",
      "0                  26,52        1900-01-01 08:40:00   NaN  NaN      NaN   \n",
      "1                  26,52        1900-01-01 08:40:01   NaN  NaN      NaN   \n",
      "2                  26,52        1900-01-01 08:40:02   NaN  NaN      NaN   \n",
      "3                  26,52        1900-01-01 08:40:03   NaN  NaN      NaN   \n",
      "4                  26,52        1900-01-01 08:40:04   NaN  NaN      NaN   \n",
      "\n",
      "  inicio_lauda fim_lauda  Ordem  \n",
      "0          NaN       NaN    NaN  \n",
      "1          NaN       NaN    NaN  \n",
      "2          NaN       NaN    NaN  \n",
      "3          NaN       NaN    NaN  \n",
      "4          NaN       NaN    NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_18192\\3716208346.py:92: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  lambda t: pd.date_range(start=f\"1900-01-01 {t}\", periods=60, freq='S')\n",
      "C:\\Temp\\ipykernel_18192\\3716208346.py:107: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  lambda row: pd.date_range(start=row['inicio_lauda_dt'], end=row['fim_lauda_dt'], freq='S'), axis=1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# PARTE 1: PROCESSAR O RUNDOWN (PDF -> DATAFRAME)\n",
    "# ==============================================================================\n",
    "\n",
    "def processar_rundown_pdf(caminho_pdf):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o est√°vel para extrair os dados do rundown (espelho) de um PDF,\n",
    "    deduzindo os blocos a partir das linhas de 'Break'.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(caminho_pdf):\n",
    "        print(f\"ERRO: O arquivo de rundown PDF n√£o foi encontrado em '{caminho_pdf}'\")\n",
    "        return None\n",
    "\n",
    "    dados_finais = []\n",
    "    bloco_numero = 1\n",
    "\n",
    "    with pdfplumber.open(caminho_pdf) as pdf:\n",
    "        for pagina in pdf.pages:\n",
    "            tabelas = pagina.extract_tables()\n",
    "            for tabela in tabelas:\n",
    "                for linha in tabela:\n",
    "                    if linha and linha[0] and isinstance(linha[0], str) and 'Break' in linha[0]:\n",
    "                        bloco_numero += 1\n",
    "                        continue\n",
    "                    \n",
    "                    if len(linha) >= 10 and linha[-1] and isinstance(linha[-1], str) and linha[-1].isdigit():\n",
    "                        dados_finais.append({\n",
    "                            'Bloco': f\"Bloco {bloco_numero:02d}\",\n",
    "                            'Tipo': linha[1],\n",
    "                            'Retranca': linha[2],\n",
    "                            'In√≠cio (Lauda)': linha[6],\n",
    "                            'Fim (Lauda)': linha[7],\n",
    "                            'Ordem': int(linha[9])\n",
    "                        })\n",
    "\n",
    "    if not dados_finais:\n",
    "        print(\"Aten√ß√£o: Nenhum dado de lauda v√°lido foi extra√≠do do PDF.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(dados_finais)\n",
    "    df = df.sort_values(by='Ordem').reset_index(drop=True)\n",
    "    \n",
    "    print(\"PDF do rundown processado com sucesso!\")\n",
    "    return df\n",
    "\n",
    "# ==============================================================================\n",
    "# PARTE 2: FAZER O MERGE COM DADOS DE AUDI√äNCIA (VERS√ÉO FINAL)\n",
    "# ==============================================================================\n",
    "\n",
    "def fazer_merge_audiencia_espelho(df_espelho_orig, caminho_audiencia):\n",
    "    \"\"\"\n",
    "    Recebe o DataFrame do espelho, o caminho do arquivo de audi√™ncia,\n",
    "    e realiza a expans√£o e o merge em n√≠vel de segundo.\n",
    "    \"\"\"\n",
    "    # --- Carregar o dataframe de audi√™ncia ---\n",
    "    try:\n",
    "        nome_arquivo, extensao = os.path.splitext(caminho_audiencia)\n",
    "        if extensao.lower() == '.csv':\n",
    "            df_minamin_orig = pd.read_csv(caminho_audiencia, sep=';', on_bad_lines='skip')\n",
    "        elif extensao.lower() in ['.xlsx', '.xls']:\n",
    "            df_minamin_orig = pd.read_excel(caminho_audiencia)\n",
    "        else:\n",
    "            print(f\"ERRO CR√çTICO: Formato de arquivo n√£o suportado '{extensao}'.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Arquivo de audi√™ncia '{caminho_audiencia}' carregado com sucesso!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO CR√çTICO ao ler o arquivo de audi√™ncia: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 1. Limpeza e Expans√£o do df_minamin ---\n",
    "    print(\"Processando dados de audi√™ncia...\")\n",
    "    df_minamin = df_minamin_orig.copy()\n",
    "    \n",
    "    # AQUI EST√Å A CORRE√á√ÉO FINAL DO FORMATO DA HORA!\n",
    "    df_minamin['HORA_base'] = pd.to_datetime(df_minamin['HORA'], format='%H:%M', errors='coerce').dt.time\n",
    "    \n",
    "    df_minamin.dropna(subset=['HORA_base'], inplace=True)\n",
    "\n",
    "    if df_minamin.empty:\n",
    "        print(\"[ALERTA CR√çTICO] O dataframe de audi√™ncia ficou vazio ap√≥s a limpeza. Verifique o arquivo de origem e o formato da hora.\")\n",
    "        return None\n",
    "    \n",
    "    # Expans√£o para segundos\n",
    "    df_minamin_segundos = df_minamin.copy()\n",
    "    df_minamin_segundos['datetime_segundo_audiencia'] = df_minamin_segundos['HORA_base'].apply(\n",
    "        lambda t: pd.date_range(start=f\"1900-01-01 {t}\", periods=60, freq='S')\n",
    "    )\n",
    "    df_minamin_segundos = df_minamin_segundos.explode('datetime_segundo_audiencia')\n",
    "\n",
    "    # --- 2. Expans√£o do df_espelho ---\n",
    "    print(\"Processando dados do espelho (rundown)...\")\n",
    "    df_espelho = df_espelho_orig.copy()\n",
    "    df_espelho.rename(columns={'In√≠cio (Lauda)': 'inicio_lauda', 'Fim (Lauda)': 'fim_lauda'}, inplace=True)\n",
    "    df_espelho['inicio_lauda_dt'] = pd.to_datetime('1900-01-01 ' + df_espelho['inicio_lauda'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "    df_espelho['fim_lauda_dt'] = pd.to_datetime('1900-01-01 ' + df_espelho['fim_lauda'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "    df_espelho.dropna(subset=['inicio_lauda_dt', 'fim_lauda_dt'], inplace=True)\n",
    "    df_espelho = df_espelho[df_espelho['fim_lauda_dt'] >= df_espelho['inicio_lauda_dt']].copy()\n",
    "    \n",
    "    if not df_espelho.empty:\n",
    "        df_espelho['datetime_segundo_espelho'] = df_espelho.apply(\n",
    "            lambda row: pd.date_range(start=row['inicio_lauda_dt'], end=row['fim_lauda_dt'], freq='S'), axis=1\n",
    "        )\n",
    "        df_espelho_segundos = df_espelho.explode('datetime_segundo_espelho')\n",
    "        df_espelho_segundos.drop_duplicates(subset=['datetime_segundo_espelho'], keep='first', inplace=True)\n",
    "    else:\n",
    "        print(\"[ALERTA] O dataframe do espelho ficou vazio ap√≥s a limpeza.\")\n",
    "        return None\n",
    "\n",
    "    # --- 3. Merge Final ---\n",
    "    print(\"Realizando o merge dos dados...\")\n",
    "    df_merged_segundos = pd.merge(\n",
    "        df_minamin_segundos,\n",
    "        df_espelho_segundos,\n",
    "        left_on='datetime_segundo_audiencia',\n",
    "        right_on='datetime_segundo_espelho',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # --- 4. Limpeza e Salvamento ---\n",
    "    colunas_para_remover = ['HORA_base', 'datetime_segundo_espelho', 'inicio_lauda_dt', 'fim_lauda_dt']\n",
    "    df_merged_segundos.drop(columns=colunas_para_remover, inplace=True, errors='ignore')\n",
    "    \n",
    "    output_path = \"audiencia_espelho_merged_segundos.csv\"\n",
    "    df_merged_segundos.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\nPROCESSO FINALIZADO! üèÜ\\nArquivo mergeado salvo em: {output_path}\")\n",
    "        \n",
    "    return df_merged_segundos\n",
    "\n",
    "# ==============================================================================\n",
    "# PONTO DE PARTIDA PRINCIPAL DO SCRIPT\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Defina os nomes dos seus arquivos de entrada aqui ---\n",
    "    ARQUIVO_RUNDOWN_PDF = \"rundown-display.pdf\"\n",
    "    ARQUIVO_AUDIENCIA = \"Realtime_clean.csv\"\n",
    "\n",
    "    print(\"Iniciando pipeline de processamento e merge...\")\n",
    "    \n",
    "    # Passo 1: Processar o PDF para obter o espelho\n",
    "    df_espelho_gerado = processar_rundown_pdf(ARQUIVO_RUNDOWN_PDF)\n",
    "    \n",
    "    # Passo 2: Se o espelho foi gerado, prosseguir com o merge\n",
    "    if df_espelho_gerado is not None:\n",
    "        df_final = fazer_merge_audiencia_espelho(df_espelho_gerado, ARQUIVO_AUDIENCIA)\n",
    "        if df_final is not None and not df_final.empty:\n",
    "            print(\"\\nVisualiza√ß√£o das primeiras linhas do dataframe final:\")\n",
    "            print(df_final.head())\n",
    "    else:\n",
    "        print(\"\\nO pipeline foi interrompido porque o espelho n√£o p√¥de ser gerado do PDF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a1f767-f35e-4ef4-96b4-8c4fe335c080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Espelho)",
   "language": "python",
   "name": "espelho"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
