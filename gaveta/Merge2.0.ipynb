{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472cafdf-4b03-4c67-b486-d811e1c2539c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline executado com sucesso!\n",
      "Arquivo final salvo como: 'Realtime_clean.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Configurações ---\n",
    "input_file = 'Realtime_bruto.csv'\n",
    "output_file = 'Realtime_clean.csv'\n",
    "\n",
    "# Dicionário que define as colunas a serem mantidas e seus novos nomes\n",
    "# Formato: { 'nome_antigo': 'nome_novo' }\n",
    "columns_to_rename_and_keep = {\n",
    "    'MIN': 'HORA',\n",
    "    'TV GAZETA': 'GLOBO',\n",
    "    'TV VITORIA': 'RECORD',\n",
    "    'TV SIM': 'SBT',\n",
    "    'CONTEÚDO TV/VÍDEO S/ REFERÊNCIA': 'Conteúdo de TV/Vídeo sem referência',\n",
    "    'TOTAL LIGADOS': 'Total Ligados Especial'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # --- Passo 1: Leitura e Limpeza de Linhas ---\n",
    "\n",
    "    # Lê o arquivo original, pulando as 6 primeiras linhas\n",
    "    df = pd.read_csv(input_file, sep=';', skiprows=6)\n",
    "\n",
    "    # Remove a última linha do DataFrame\n",
    "    # Usamos .iloc[:-1] para selecionar todas as linhas, exceto a última\n",
    "    df_cleaned_rows = df.iloc[:-1].copy()\n",
    "\n",
    "\n",
    "    # --- Passo 2: Seleção e Renomeação de Colunas ---\n",
    "\n",
    "    # Pega a lista das colunas originais que queremos manter\n",
    "    original_columns_to_keep = list(columns_to_rename_and_keep.keys())\n",
    "\n",
    "    # Filtra o DataFrame, mantendo apenas as colunas desejadas\n",
    "    df_selected_cols = df_cleaned_rows[original_columns_to_keep]\n",
    "\n",
    "    # Renomeia as colunas do DataFrame já filtrado\n",
    "    df_final = df_selected_cols.rename(columns=columns_to_rename_and_keep)\n",
    "\n",
    "\n",
    "    # --- Passo 3: Salvar o Resultado ---\n",
    "\n",
    "    # Salva o DataFrame final, limpo e formatado, em um novo arquivo CSV\n",
    "    df_final.to_csv(output_file, sep=';', index=False, encoding='utf-8')\n",
    "\n",
    "    print(\"✅ Pipeline executado com sucesso!\")\n",
    "    print(f\"Arquivo final salvo como: '{output_file}'\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERRO: O arquivo de entrada '{input_file}' não foi encontrado.\")\n",
    "except KeyError as e:\n",
    "    print(f\"❌ ERRO: Uma ou mais colunas para renomear não foram encontradas no arquivo: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Ocorreu um erro inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e4b692-9af4-4997-8b70-e910641441e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando pipeline de processamento e merge...\n",
      "PDF do rundown processado com sucesso!\n",
      "Arquivo de audiência 'Realtime_clean.csv' carregado com sucesso!\n",
      "Expandindo dados de audiência para segundos...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'HORA'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Projetos\\Espelho\\espelho\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'HORA'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 143\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Passo 2: Se o espelho foi gerado, prosseguir com o merge\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df_espelho_gerado \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[43mfazer_merge_audiencia_espelho\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_espelho_gerado\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mARQUIVO_AUDIENCIA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mO pipeline foi interrompido porque o espelho não pôde ser gerado do PDF.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 71\u001b[0m, in \u001b[0;36mfazer_merge_audiencia_espelho\u001b[1;34m(df_espelho_orig, caminho_audiencia)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpandindo dados de audiência para segundos...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m df_minamin \u001b[38;5;241m=\u001b[39m df_minamin_orig\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 71\u001b[0m df_minamin[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHORA_base\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf_minamin\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHORA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m     73\u001b[0m minamin_segundos_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df_minamin\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[1;32mD:\\Projetos\\Espelho\\espelho\\lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mD:\\Projetos\\Espelho\\espelho\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3817\u001b[0m     ):\n\u001b[0;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'HORA'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# PARTE 1: NOSSA FUNÇÃO PARA PROCESSAR O RUNDOWN (PDF -> DATAFRAME)\n",
    "# ==============================================================================\n",
    "\n",
    "def processar_rundown_pdf(caminho_pdf):\n",
    "    \"\"\"\n",
    "    Função estável para extrair os dados do rundown,\n",
    "    deduzindo os blocos a partir das linhas de 'Break'.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(caminho_pdf):\n",
    "        print(f\"ERRO: O arquivo de rundown PDF não foi encontrado em '{caminho_pdf}'\")\n",
    "        return None\n",
    "\n",
    "    dados_finais = []\n",
    "    bloco_numero = 1\n",
    "\n",
    "    with pdfplumber.open(caminho_pdf) as pdf:\n",
    "        for pagina in pdf.pages:\n",
    "            tabelas = pagina.extract_tables()\n",
    "            for tabela in tabelas:\n",
    "                for linha in tabela:\n",
    "                    if linha and linha[0] and isinstance(linha[0], str) and 'Break' in linha[0]:\n",
    "                        bloco_numero += 1\n",
    "                        continue\n",
    "                    \n",
    "                    if len(linha) >= 10 and linha[-1] and isinstance(linha[-1], str) and linha[-1].isdigit():\n",
    "                        dados_finais.append({\n",
    "                            'Bloco': f\"Bloco {bloco_numero:02d}\",\n",
    "                            'Tipo': linha[1],\n",
    "                            'Retranca': linha[2],\n",
    "                            'Início (Lauda)': linha[6],\n",
    "                            'Fim (Lauda)': linha[7],\n",
    "                            'Ordem': int(linha[9])\n",
    "                        })\n",
    "\n",
    "    if not dados_finais:\n",
    "        print(\"Atenção: Nenhum dado de lauda válido foi extraído do PDF.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(dados_finais)\n",
    "    df = df.sort_values(by='Ordem').reset_index(drop=True)\n",
    "    \n",
    "    print(\"PDF do rundown processado com sucesso!\")\n",
    "    return df\n",
    "\n",
    "# ==============================================================================\n",
    "# PARTE 2: SEU SCRIPT DE MERGE, AGORA INTEGRADO\n",
    "# ==============================================================================\n",
    "\n",
    "def fazer_merge_audiencia_espelho(df_espelho_orig, caminho_audiencia):\n",
    "    \"\"\"\n",
    "    Recebe o DataFrame do espelho e o caminho do arquivo de audiência,\n",
    "    e realiza a expansão e o merge em nível de segundo.\n",
    "    \"\"\"\n",
    "    # --- Carregar o dataframe de audiência ---\n",
    "    try:\n",
    "        df_minamin_orig = pd.read_excel(caminho_audiencia)\n",
    "        print(f\"Arquivo de audiência '{caminho_audiencia}' carregado com sucesso!\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERRO CRÍTICO: Arquivo de audiência não encontrado - {caminho_audiencia}\")\n",
    "        return\n",
    "\n",
    "    # --- 1. Expandir df_minamin para segundos ---\n",
    "    print(\"Expandindo dados de audiência para segundos...\")\n",
    "    df_minamin = df_minamin_orig.copy()\n",
    "    df_minamin['HORA_base'] = pd.to_datetime(df_minamin['HORA'], format='%H:%M:%S', errors='coerce').dropna()\n",
    "    \n",
    "    minamin_segundos_list = []\n",
    "    for _, row in df_minamin.iterrows():\n",
    "        minuto_base = row['HORA_base']\n",
    "        for segundo_offset in range(60):\n",
    "            new_row_data = row.to_dict()\n",
    "            new_row_data['datetime_segundo_audiencia'] = pd.to_datetime('1900-01-01 ' + (minuto_base + pd.Timedelta(seconds=segundo_offset)).strftime('%H:%M:%S'))\n",
    "            minamin_segundos_list.append(new_row_data)\n",
    "\n",
    "    df_minamin_segundos = pd.DataFrame(minamin_segundos_list)\n",
    "    \n",
    "    # --- 2. Expandir df_espelho para segundos ---\n",
    "    print(\"Expandindo dados do espelho (rundown) para segundos...\")\n",
    "    df_espelho = df_espelho_orig.copy()\n",
    "    df_espelho.rename(columns={'Início (Lauda)': 'inicio_lauda', 'Fim (Lauda)': 'fim_lauda'}, inplace=True)\n",
    "    \n",
    "    df_espelho['inicio_lauda_dt'] = pd.to_datetime('1900-01-01 ' + df_espelho['inicio_lauda'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "    df_espelho['fim_lauda_dt'] = pd.to_datetime('1900-01-01 ' + df_espelho['fim_lauda'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "    df_espelho.dropna(subset=['inicio_lauda_dt', 'fim_lauda_dt'], inplace=True)\n",
    "    df_espelho = df_espelho[df_espelho['fim_lauda_dt'] >= df_espelho['inicio_lauda_dt']]\n",
    "\n",
    "    espelho_segundos_list = []\n",
    "    for _, row in df_espelho.iterrows():\n",
    "        for segundo_ts in pd.date_range(row['inicio_lauda_dt'], row['fim_lauda_dt'], freq='S'):\n",
    "            new_row_data = row.to_dict()\n",
    "            new_row_data['datetime_segundo_espelho'] = segundo_ts\n",
    "            espelho_segundos_list.append(new_row_data)\n",
    "    \n",
    "    if not espelho_segundos_list:\n",
    "        print(\"ERRO: Não foi possível expandir o espelho para segundos.\")\n",
    "        return\n",
    "        \n",
    "    df_espelho_segundos = pd.DataFrame(espelho_segundos_list)\n",
    "    df_espelho_segundos.drop_duplicates(subset=['datetime_segundo_espelho'], keep='first', inplace=True)\n",
    "    \n",
    "    # --- 3. Merge em Nível de Segundo ---\n",
    "    print(\"Realizando merge da audiência com o espelho...\")\n",
    "    df_merged_segundos = pd.merge(\n",
    "        df_minamin_segundos,\n",
    "        df_espelho_segundos,\n",
    "        left_on='datetime_segundo_audiencia',\n",
    "        right_on='datetime_segundo_espelho',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # --- 4. Limpeza e Salvamento ---\n",
    "    colunas_para_remover = [\n",
    "        'HORA_base', 'datetime_segundo_espelho', 'inicio_lauda_dt', 'fim_lauda_dt'\n",
    "    ]\n",
    "    df_merged_segundos.drop(columns=colunas_para_remover, inplace=True, errors='ignore')\n",
    "    \n",
    "    output_path = \"audiencia_espelho_merged_segundos_2.0.csv\"\n",
    "    df_merged_segundos.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nPROCESSO FINALIZADO! 🏆\\nArquivo mergeado salvo em: {output_path}\")\n",
    "    \n",
    "# ==============================================================================\n",
    "# PONTO DE PARTIDA PRINCIPAL DO SCRIPT\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Defina os nomes dos seus arquivos de entrada aqui ---\n",
    "    ARQUIVO_RUNDOWN_PDF = \"rundown-display.pdf\"\n",
    "    ARQUIVO_AUDIENCIA = \"Realtime_clean.xlsx\"\n",
    "\n",
    "    print(\"Iniciando pipeline de processamento e merge...\")\n",
    "    \n",
    "    # Passo 1: Processar o PDF para obter o espelho\n",
    "    df_espelho_gerado = processar_rundown_pdf(ARQUIVO_RUNDOWN_PDF)\n",
    "    \n",
    "    # Passo 2: Se o espelho foi gerado, prosseguir com o merge\n",
    "    if df_espelho_gerado is not None:\n",
    "        fazer_merge_audiencia_espelho(df_espelho_gerado, ARQUIVO_AUDIENCIA)\n",
    "    else:\n",
    "        print(\"\\nO pipeline foi interrompido porque o espelho não pôde ser gerado do PDF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de9b4d-98a0-43ab-b729-1e831095608d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Espelho)",
   "language": "python",
   "name": "espelho"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
